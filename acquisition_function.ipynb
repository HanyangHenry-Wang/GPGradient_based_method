{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from botorch.exceptions import UnsupportedError\n",
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.models.model import Model\n",
    "from botorch.utils.constants import get_constants_like\n",
    "from botorch.utils.probability import MVNXPB\n",
    "from botorch.utils.probability.utils import (\n",
    "    log_ndtr as log_Phi,\n",
    "    log_phi,\n",
    "    log_prob_normal_in,\n",
    "    ndtr as Phi,\n",
    "    phi,\n",
    ")\n",
    "from botorch.utils.safe_math import log1mexp, logmeanexp\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "_sqrt_2pi = math.sqrt(2 * math.pi)\n",
    "# the following two numbers are needed for _log_ei_helper\n",
    "_neg_inv_sqrt2 = -(2**-0.5)\n",
    "_log_sqrt_pi_div_2 = math.log(math.pi / 2) / 2\n",
    "\n",
    "\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.models.model import Model\n",
    "from typing import Any, Optional, Union\n",
    "from torch import Tensor\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "# from botorch.acquisition import *\n",
    "from abc import ABC\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "import botorch.acquisition as temp\n",
    "import botorch.acquisition \n",
    "from botorch.utils.probability.utils import log_phi, log_ndtr,ndtr as Phi, phi\n",
    "\n",
    "# def _gamma(\n",
    "#     mean: Tensor, sigma: Tensor, fstar: Tensor, maximize: bool\n",
    "# ) -> Tensor:\n",
    "#     \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "#     u = (fstar - mean) / sigma\n",
    "#     return u if maximize else -u\n",
    "\n",
    "\n",
    "\n",
    "class AnalyticAcquisitionFunction(AcquisitionFunction, ABC):\n",
    "    r\"\"\"\n",
    "    Base class for analytic acquisition functions.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        r\"\"\"Base constructor for analytic acquisition functions.\n",
    "\n",
    "        Args:\n",
    "            model: A fitted single-outcome model.\n",
    "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
    "                a PosteriorTransform that transforms the multi-output posterior into a\n",
    "                single-output posterior is required.\n",
    "        \"\"\"\n",
    "        super().__init__(model=model)\n",
    "        posterior_transform = self._deprecate_acqf_objective(\n",
    "            posterior_transform=posterior_transform,\n",
    "            objective=kwargs.get(\"objective\"),\n",
    "        )\n",
    "        if posterior_transform is None:\n",
    "            if model.num_outputs != 1:\n",
    "                raise UnsupportedError(\n",
    "                    \"Must specify a posterior transform when using a \"\n",
    "                    \"multi-output model.\"\n",
    "                )\n",
    "        else:\n",
    "            if not isinstance(posterior_transform, PosteriorTransform):\n",
    "                raise UnsupportedError(\n",
    "                    \"AnalyticAcquisitionFunctions only support PosteriorTransforms.\"\n",
    "                )\n",
    "        self.posterior_transform = posterior_transform\n",
    "\n",
    "    def set_X_pending(self, X_pending: Optional[Tensor] = None) -> None:\n",
    "        raise UnsupportedError(\n",
    "            \"Analytic acquisition functions do not account for X_pending yet.\"\n",
    "        )\n",
    "\n",
    "    def _mean_and_sigma(\n",
    "        self, X: Tensor, compute_sigma: bool = True, min_var: float = 1e-12\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Computes the first and second moments of the model posterior.\n",
    "\n",
    "        Args:\n",
    "            X: `batch_shape x q x d`-dim Tensor of model inputs.\n",
    "            compute_sigma: Boolean indicating whether or not to compute the second\n",
    "                moment (default: True).\n",
    "            min_var: The minimum value the variance is clamped too. Should be positive.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of tensors containing the first and second moments of the model\n",
    "            posterior. Removes the last two dimensions if they have size one. Only\n",
    "            returns a single tensor of means if compute_sigma is True.\n",
    "        \"\"\"\n",
    "        self.to(device=X.device)  # ensures buffers / parameters are on the same device\n",
    "        posterior = self.model.posterior(\n",
    "            X=X, posterior_transform=self.posterior_transform\n",
    "        )\n",
    "        mean = posterior.mean.squeeze(-2).squeeze(-1)  # removing redundant dimensions\n",
    "        if not compute_sigma:\n",
    "            return mean, None\n",
    "        sigma = posterior.variance.clamp_min(min_var).sqrt().view(mean.shape)\n",
    "        return mean, sigma\n",
    "\n",
    "\n",
    "# --------------- Helper functions for analytic acquisition functions. ---------------\n",
    "\n",
    "\n",
    "def _scaled_improvement(\n",
    "    mean: Tensor, sigma: Tensor, best_f: Tensor, maximize: bool\n",
    ") -> Tensor:\n",
    "    \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "    u = (mean - best_f) / sigma\n",
    "    return u if maximize else -u\n",
    "\n",
    "\n",
    "def _gamma(\n",
    "    mean: Tensor, sigma: Tensor, fstar: Tensor, maximize: bool\n",
    ") -> Tensor:\n",
    "    \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "    u = (fstar - mean) / sigma\n",
    "    return u if maximize else -u\n",
    "\n",
    "\n",
    "def _ei_helper(u: Tensor) -> Tensor:\n",
    "    \"\"\"Computes phi(u) + u * Phi(u), where phi and Phi are the standard normal\n",
    "    pdf and cdf, respectively. This is used to compute Expected Improvement.\n",
    "    \"\"\"\n",
    "    return phi(u) + u * Phi(u)\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.functional import softplus\n",
    "import math\n",
    "\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "def cauchy(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes a Lorentzian, i.e. an un-normalized Cauchy density function.\"\"\"\n",
    "    return 1 / (1 + x.square())\n",
    "\n",
    "def fatplus(x: Tensor, tau: Union[float, Tensor] = 0.5) -> Tensor:\n",
    "    \"\"\"Computes a fat-tailed approximation to `ReLU(x) = max(x, 0)` by linearly\n",
    "    combining a regular softplus function and the density function of a Cauchy\n",
    "    distribution. The coefficient `alpha` of the Cauchy density is chosen to guarantee\n",
    "    monotonicity and convexity.\n",
    "\n",
    "    Args:\n",
    "        x: A Tensor on whose values to compute the smoothed function.\n",
    "        tau: Temperature parameter controlling the smoothness of the approximation.\n",
    "\n",
    "    Returns:\n",
    "        A Tensor of values of the fat-tailed softplus.\n",
    "    \"\"\"\n",
    "\n",
    "    def _fatplus(x: Tensor) -> Tensor:\n",
    "        alpha = 1e-1  # guarantees monotonicity and convexity (TODO: ref + Lemma 4)\n",
    "        return softplus(x) + alpha * cauchy(x)\n",
    "\n",
    "    return tau * _fatplus(x / tau)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class ExpectedImprovement(AnalyticAcquisitionFunction):\n",
    "    r\"\"\"Single-outcome Expected Improvement (analytic).\n",
    "\n",
    "    Computes classic Expected Improvement over the current best observed value,\n",
    "    using the analytic formula for a Normal posterior distribution. Unlike the\n",
    "    MC-based acquisition functions, this relies on the posterior at single test\n",
    "    point being Gaussian (and require the posterior to implement `mean` and\n",
    "    `variance` properties). Only supports the case of `q=1`. The model must be\n",
    "    single-outcome.\n",
    "\n",
    "    `EI(x) = E(max(f(x) - best_f, 0)),`\n",
    "\n",
    "    where the expectation is taken over the value of stochastic function `f` at `x`.\n",
    "\n",
    "    Example:\n",
    "        >>> model = SingleTaskGP(train_X, train_Y)\n",
    "        >>> EI = ExpectedImprovement(model, best_f=0.2)\n",
    "        >>> ei = EI(test_X)\n",
    "\n",
    "    NOTE: It is *strongly* recommended to use LogExpectedImprovement instead of regular\n",
    "    EI, because it solves the vanishing gradient problem by taking special care of\n",
    "    numerical computations and can lead to substantially improved BO performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        maximize: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"Single-outcome Expected Improvement (analytic).\n",
    "\n",
    "        Args:\n",
    "            model: A fitted single-outcome model.\n",
    "            best_f: Either a scalar or a `b`-dim Tensor (batch mode) representing\n",
    "                the best function value observed so far (assumed noiseless).\n",
    "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
    "                a PosteriorTransform that transforms the multi-output posterior into a\n",
    "                single-output posterior is required.\n",
    "            maximize: If True, consider the problem a maximization problem.\n",
    "        \"\"\"\n",
    "        super().__init__(model=model, posterior_transform=posterior_transform, **kwargs)\n",
    "        self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.maximize = maximize\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "\n",
    "        r\"\"\"Evaluate Expected Improvement on the candidate set X.\n",
    "\n",
    "        Args:\n",
    "            X: A `(b1 x ... bk) x 1 x d`-dim batched tensor of `d`-dim design points.\n",
    "                Expected Improvement is computed for each point individually,\n",
    "                i.e., what is considered are the marginal posteriors, not the\n",
    "                joint.\n",
    "\n",
    "        Returns:\n",
    "            A `(b1 x ... bk)`-dim tensor of Expected Improvement values at the\n",
    "            given design points `X`.\n",
    "        \"\"\"\n",
    "        mean, sigma = self._mean_and_sigma(X)\n",
    "        u = _scaled_improvement(mean, sigma, self.best_f, self.maximize)\n",
    "        return sigma * _ei_helper(u)\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "class Fstar_pdf_GradientEnhanced(AnalyticAcquisitionFunction):\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        #best_f: Union[float, Tensor],\n",
    "        fstar: Union[float, Tensor],\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        maximize: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        super().__init__(model=model, posterior_transform=posterior_transform, **kwargs)\n",
    "        #self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"fstar\", torch.as_tensor(fstar))\n",
    "        self.maximize = maximize\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "       \n",
    "        mean, sigma = self._mean_and_sigma(X)\n",
    "        gamma = _gamma(mean, sigma, self.fstar, self.maximize)\n",
    "        \n",
    "        \n",
    "        # use fat softplus to make the value smaller than -15 be -15\n",
    "        max_dis = 20.\n",
    "        gamma = fatplus(gamma+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "        gamma = - (fatplus(-gamma+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "        \n",
    "        part1 = log_phi(gamma)\n",
    "        \n",
    "        # part 2 calculation\n",
    "        D = X.shape[-1]\n",
    "        mean_d, variance_d = self.model.posterior_derivative(X)\n",
    "        \n",
    "        logpdf_total = torch.zeros(X.shape[0])\n",
    "        \n",
    "        for d in range(D):\n",
    "            mean_list = mean_d[:,d]\n",
    "            sigma_list = torch.sqrt(variance_d[:,d,d])\n",
    "            \n",
    "            u = (torch.as_tensor(0.)-mean_list)/sigma_list\n",
    "            \n",
    "            \n",
    "            # use fat softplus to make the value smaller than -15 be -15\n",
    "            max_dis = 20.\n",
    "            u = fatplus(u+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "            u = - (fatplus(-u+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "\n",
    "            \n",
    "            pdf_temp = log_phi(u)  #log_phi(u)  phi(u)\n",
    "            \n",
    "            logpdf_total += pdf_temp\n",
    "            \n",
    "            # print(pdf_temp)\n",
    "            \n",
    "        #print('log pdf is: ', pdf_total)\n",
    "        # print('part 2: ',logpdf_total)\n",
    "        # print(logpdf_total.shape)\n",
    "        \n",
    "        part2 = logpdf_total\n",
    "       \n",
    "        return part1  + part2\n",
    "    \n",
    "\n",
    "\n",
    "class Fstar_pdf_GradientEnhanced_fantasy(AnalyticAcquisitionFunction):\n",
    "   \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        #best_f: Union[float, Tensor],\n",
    "        fstar: Union[float, Tensor],\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        maximize: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        super().__init__(model=model, posterior_transform=posterior_transform, **kwargs)\n",
    "        #self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"fstar\", torch.as_tensor(fstar))\n",
    "        self.maximize = maximize\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "       \n",
    "        mean, sigma = self._mean_and_sigma(X)\n",
    "        gamma = _gamma(mean, sigma, self.fstar, self.maximize)\n",
    "        \n",
    "        \n",
    "        # use fat softplus to make the value smaller than -15 be -15\n",
    "        max_dis = 20.\n",
    "        gamma = fatplus(gamma+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "        gamma = - (fatplus(-gamma+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "        \n",
    "        part1 = log_phi(gamma)\n",
    "        \n",
    "        # part 2 calculation\n",
    "        logpdf_total = torch.zeros(X.shape[0])\n",
    "        D = X.shape[-1]\n",
    "        \n",
    "        \n",
    "        for ii in range(X.shape[0]):\n",
    "            \n",
    "            x_temp = X[ii]\n",
    "            \n",
    "            logpdf_temp = 0.\n",
    "            \n",
    "            x_temp = x_temp.reshape(1,1,D)\n",
    "            \n",
    "            model_temp = self.model.get_fantasy_model(x_temp.reshape(-1,D), torch.tensor([self.fstar.item()]).reshape(-1,1))\n",
    "            model_temp.N = self.model.N+1\n",
    "            model_temp.train_targets = model_temp.train_targets.reshape(model_temp.N)\n",
    "            \n",
    "            mean_d, variance_d = model_temp.posterior_derivative(x_temp.reshape(-1,1,D))\n",
    "        \n",
    "       \n",
    "            for d in range(D):\n",
    "                mean_list = mean_d[:,d]\n",
    "                sigma_list = torch.sqrt(variance_d[:,d,d])\n",
    "                \n",
    "                u = (torch.as_tensor(0.)-mean_list)/sigma_list\n",
    "                \n",
    "                \n",
    "                # use fat softplus to make the value smaller than -15 be -15\n",
    "                max_dis = 20.\n",
    "                u = fatplus(u+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "                u = - (fatplus(-u+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "\n",
    "                \n",
    "                pdf_temp = log_phi(u)  #log_phi(u)  phi(u)\n",
    "                \n",
    "                logpdf_temp += pdf_temp.item()\n",
    "                \n",
    "            logpdf_total[ii] = logpdf_temp\n",
    "            \n",
    "            # print(pdf_temp)\n",
    "            \n",
    "        #print('log pdf is: ', pdf_total)\n",
    "        # print('part 2: ',logpdf_total)\n",
    "        # print(logpdf_total.shape)\n",
    "        \n",
    "        part2 = logpdf_total\n",
    "       \n",
    "        return part1  + part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Ackley,Beale,Branin,Rosenbrock,SixHumpCamel,Hartmann,Powell,DixonPrice,Levy,StyblinskiTang,Griewank\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import unnormalize,normalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "import matplotlib.pyplot as plt\n",
    "from model import DerivativeExactGPSEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "torch.set_default_dtype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "iter_num = 30\n",
    "\n",
    "function = Levy(dim=2,negate=True)\n",
    "fstar= 0.\n",
    "bounds=function.bounds.to(device)\n",
    "dim = bounds.shape[1]\n",
    "\n",
    "standard_bounds=torch.tensor([0.,1.]*dim).reshape(-1,2).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanyang/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/optim/optimize.py:369: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n",
      "/home/hanyang/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/optim/optimize.py:393: RuntimeWarning: Optimization failed on the second try, after generating a new set of initial conditions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5791, 0.5322]])\n",
      "tensor(-2.9557)\n",
      "-0.4344234176914185\n",
      "tensor([[0.5300, 0.5425]])\n",
      "tensor(-2.9772)\n",
      "-0.1474072235969596\n",
      "tensor([[1., 0.]])\n",
      "tensor(-3.2050)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5057, 0.4963]])\n",
      "tensor(-2.7791)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5147, 0.2444]])\n",
      "tensor(-3.4794)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5673, 0.7510]])\n",
      "tensor(-3.7009)\n",
      "-0.1474072235969596\n",
      "tensor([[0.4294, 0.0000]])\n",
      "tensor(-4.5426)\n",
      "-0.1474072235969596\n",
      "tensor([[0.4035, 0.3531]])\n",
      "tensor(-2.9419)\n",
      "-0.1474072235969596\n",
      "tensor([[0.6213, 0.2835]])\n",
      "tensor(-2.7978)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5911, 0.9560]])\n",
      "tensor(-2.9128)\n",
      "-0.1474072235969596\n",
      "tensor([[0., 0.]])\n",
      "tensor(-3.0761)\n",
      "-0.1474072235969596\n",
      "tensor([[0.0365, 0.5428]])\n",
      "tensor(-3.0757)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5060, 0.8412]])\n",
      "tensor(-3.0490)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5535, 0.4912]])\n",
      "tensor(-2.9397)\n",
      "-0.1474072235969596\n",
      "tensor([[0.9294, 0.5656]])\n",
      "tensor(-3.8308)\n",
      "-0.1474072235969596\n",
      "tensor([[0.6459, 0.8525]])\n",
      "tensor(-3.4208)\n",
      "-0.1474072235969596\n",
      "tensor([[0.3797, 0.7402]])\n",
      "tensor(-6.5234)\n",
      "-0.1474072235969596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanyang/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/optim/optimize.py:369: RuntimeWarning: Optimization failed in `gen_candidates_scipy` with the following warning(s):\n",
      "[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL_TERMINATION_IN_LNSRCH.')]\n",
      "Trying again with a new set of initial conditions.\n",
      "  warnings.warn(first_warn_msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5183, 0.6706]])\n",
      "tensor(-5.1334)\n",
      "-0.1474072235969596\n",
      "tensor([[0.0000, 0.8707]])\n",
      "tensor(-9.1095)\n",
      "-0.1474072235969596\n",
      "tensor([[1.0000, 0.3458]])\n",
      "tensor(-10.6085)\n",
      "-0.1474072235969596\n",
      "tensor([[0.7207, 0.4546]])\n",
      "tensor(-2.9920)\n",
      "-0.1474072235969596\n",
      "tensor([[0.7204, 0.0000]])\n",
      "tensor(-5.0406)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5527, 0.4086]])\n",
      "tensor(-7.8168)\n",
      "-0.1474072235969596\n",
      "tensor([[0.1956, 0.4339]])\n",
      "tensor(-10.3906)\n",
      "-0.1474072235969596\n",
      "tensor([[0.7858, 0.1830]])\n",
      "tensor(-9.3506)\n",
      "-0.1474072235969596\n",
      "tensor([[0.5387, 0.5625]])\n",
      "tensor(-4.5606)\n",
      "-0.05577618912907121\n",
      "tensor([[1., 1.]])\n",
      "tensor(-28.0602)\n",
      "-0.05577618912907121\n",
      "tensor([[0.4915, 0.9625]])\n",
      "tensor(-14.3531)\n",
      "-0.05577618912907121\n",
      "tensor([[0.5325, 0.5531]])\n",
      "tensor(-4.3839)\n",
      "-0.05577618912907121\n",
      "tensor([[0.7568, 0.5382]])\n",
      "tensor(-26.5450)\n",
      "-0.05577618912907121\n"
     ]
    }
   ],
   "source": [
    "EI_record = []\n",
    "\n",
    "for exp in range(1):\n",
    "\n",
    "  print(exp)\n",
    "  torch.manual_seed(exp)\n",
    "\n",
    "  train_x_standard = get_initial_points(dim,4*dim,exp).to(device)\n",
    "  train_x = unnormalize(train_x_standard, bounds).reshape(-1,dim)\n",
    "  train_obj = function(train_x).unsqueeze(-1)\n",
    "\n",
    "  best_value = train_obj.max().item()\n",
    "  best_value_holder = [best_value]\n",
    "\n",
    "  for i in range (iter_num):\n",
    "\n",
    "    train_x_standard = normalize(train_x, bounds).to(device)\n",
    "    train_obj_standard = (train_obj - train_obj.mean()) / train_obj.std()\n",
    "    \n",
    "    fstar_standard = (fstar - train_obj.mean()) / train_obj.std()\n",
    "\n",
    "    torch.manual_seed(exp+iter_num)\n",
    "    model = DerivativeExactGPSEModel(dim,train_x_standard, train_obj_standard).to(device)\n",
    "\n",
    "    #model = SingleTaskGP(train_x_standard, train_obj_standard).to(device)\n",
    "    \n",
    "\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model) .to(device)\n",
    "\n",
    "    torch.manual_seed(exp+iter_num)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    \n",
    "    # print('lengthscale: ',model.covar_module.base_kernel.lengthscale)\n",
    "    # print('variance: ',model.covar_module.outputscale )\n",
    "    \n",
    "    torch.manual_seed(exp+iter_num)\n",
    "    \n",
    "    #AF = ExpectedImprovement(model=model, best_f=train_obj_standard.max().item()) .to(device)\n",
    "    \n",
    "    #AF = Fstar_pdf_GradientEnhanced(model=model, fstar=fstar_standard) .to(device)\n",
    "\n",
    "    AF = Fstar_pdf_GradientEnhanced_fantasy(model=model, fstar=fstar_standard) .to(device)\n",
    "\n",
    "    new_point_analytic, af_val = optimize_acqf(\n",
    "        acq_function=AF,\n",
    "        bounds=standard_bounds .to(device),\n",
    "        q=1,\n",
    "        num_restarts=3*dim,\n",
    "        raw_samples=30*dim,\n",
    "        options={},\n",
    "    )\n",
    "\n",
    "    print(new_point_analytic)\n",
    "    print(af_val)\n",
    "\n",
    "    next_x = unnormalize(new_point_analytic, bounds).reshape(-1,dim)\n",
    "    new_obj = function(next_x).unsqueeze(-1) .to(device)\n",
    "\n",
    "\n",
    "    train_x = torch.cat((train_x, next_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    best_value = train_obj.max().item()\n",
    "    best_value_holder.append(best_value)\n",
    "\n",
    "    print(best_value_holder[-1])\n",
    "\n",
    "  best_value_holder = np.array(best_value_holder)\n",
    "  EI_record.append(best_value_holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EI:  [array([-1.1237396 , -1.1237396 , -0.80903737, -0.80903737, -0.80903737,\n",
      "       -0.80903737, -0.80903737, -0.80903737, -0.80903737, -0.80903737,\n",
      "       -0.80903737, -0.80903737, -0.27803632, -0.27803632, -0.27803632,\n",
      "       -0.27803632, -0.27803632, -0.27803632, -0.27803632, -0.27803632,\n",
      "       -0.15624171, -0.15624171, -0.15624171, -0.15624171, -0.15624171,\n",
      "       -0.15624171, -0.15624171, -0.15624171, -0.15624171, -0.15624171,\n",
      "       -0.15624171])]\n"
     ]
    }
   ],
   "source": [
    "print('EI: ',EI_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf gradient:  [array([-1.1237396 , -0.79409405, -0.4033131 , -0.4033131 , -0.38387134,\n",
      "       -0.38387134, -0.38387134, -0.38387134, -0.38387134, -0.38387134,\n",
      "       -0.38387134, -0.38387134, -0.38387134, -0.38387134, -0.38387134,\n",
      "       -0.38387134, -0.38387134, -0.38387134, -0.38387134, -0.38387134,\n",
      "       -0.38387134, -0.38387134, -0.38387134, -0.38387134, -0.38387134,\n",
      "       -0.38387134, -0.38387134, -0.38387134, -0.38387134, -0.38387134,\n",
      "       -0.38387134])]\n"
     ]
    }
   ],
   "source": [
    "print('pdf gradient: ',EI_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf gradient fansaty:  [array([-1.1237396 , -0.43442342, -0.14740722, -0.14740722, -0.14740722,\n",
      "       -0.14740722, -0.14740722, -0.14740722, -0.14740722, -0.14740722,\n",
      "       -0.14740722, -0.14740722, -0.14740722, -0.14740722, -0.14740722,\n",
      "       -0.14740722, -0.14740722, -0.14740722, -0.14740722, -0.14740722,\n",
      "       -0.14740722, -0.14740722, -0.14740722, -0.14740722, -0.14740722,\n",
      "       -0.14740722, -0.05577619, -0.05577619, -0.05577619, -0.05577619,\n",
      "       -0.05577619])]\n"
     ]
    }
   ],
   "source": [
    "print('pdf gradient fansaty: ',EI_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "known_boundary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
