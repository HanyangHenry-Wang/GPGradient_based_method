{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Ackley,Beale,Branin,Rosenbrock,SixHumpCamel,Hartmann,Powell,DixonPrice,Levy,StyblinskiTang,Griewank\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import unnormalize,normalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "from botorch.test_functions import Ackley,Beale,Branin,Rosenbrock,SixHumpCamel,Hartmann,Rastrigin,Powell,DixonPrice,Levy,StyblinskiTang,Griewank\n",
    "\n",
    "\n",
    "\n",
    "from model import DerivativeExactGPSEModel,DerivativeExactGPSEModel_2\n",
    "# from acquisition import (MES_KnownOptimum,TruncatedExpectedImprovement,ExpectedImprovement,Fstar_pdf,\n",
    "#                             Fstar_pdf_GradientEnhanced,Fstar_pdf_GradientEnhanced_fantasy,\n",
    "#                             TruncatedExpectedImprovement_GradientEnhanced_fantasy,\n",
    "#                             TruncatedExpectedImprovement_GradientEnhanced_fantasy_2,\n",
    "#                             TruncatedExpectedImprovement_GradientEnhanced_fantasy_parallel)\n",
    "from acquisition import  My_acquisition_opt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "r\"\"\"\n",
    "Analytic Acquisition Functions that evaluate the posterior without performing\n",
    "Monte-Carlo sampling.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from botorch.acquisition.acquisition import AcquisitionFunction\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from botorch.exceptions import UnsupportedError\n",
    "from botorch.models.gp_regression import FixedNoiseGP\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.models.model import Model\n",
    "from botorch.utils.constants import get_constants_like\n",
    "from botorch.utils.probability import MVNXPB\n",
    "from botorch.utils.probability.utils import (\n",
    "    log_ndtr as log_Phi,\n",
    "    log_phi,\n",
    "    log_prob_normal_in,\n",
    "    ndtr as Phi,\n",
    "    phi,\n",
    ")\n",
    "from botorch.utils.safe_math import log1mexp, logmeanexp\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "_sqrt_2pi = math.sqrt(2 * math.pi)\n",
    "# the following two numbers are needed for _log_ei_helper\n",
    "_neg_inv_sqrt2 = -(2**-0.5)\n",
    "_log_sqrt_pi_div_2 = math.log(math.pi / 2) / 2\n",
    "\n",
    "\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.models.model import Model\n",
    "from typing import Any, Optional, Union\n",
    "from torch import Tensor\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from botorch.utils.transforms import convert_to_target_pre_hook, t_batch_mode_transform\n",
    "# from botorch.acquisition import *\n",
    "from abc import ABC\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "import botorch.acquisition as temp\n",
    "import botorch.acquisition \n",
    "from botorch.utils.probability.utils import log_phi, log_ndtr,ndtr as Phi, phi\n",
    "\n",
    "# def _gamma(\n",
    "#     mean: Tensor, sigma: Tensor, fstar: Tensor, maximize: bool\n",
    "# ) -> Tensor:\n",
    "#     \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "#     u = (fstar - mean) / sigma\n",
    "#     return u if maximize else -u\n",
    "\n",
    "\n",
    "\n",
    "class AnalyticAcquisitionFunction(AcquisitionFunction, ABC):\n",
    "    r\"\"\"\n",
    "    Base class for analytic acquisition functions.\n",
    "\n",
    "    :meta private:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        r\"\"\"Base constructor for analytic acquisition functions.\n",
    "\n",
    "        Args:\n",
    "            model: A fitted single-outcome model.\n",
    "            posterior_transform: A PosteriorTransform. If using a multi-output model,\n",
    "                a PosteriorTransform that transforms the multi-output posterior into a\n",
    "                single-output posterior is required.\n",
    "        \"\"\"\n",
    "        super().__init__(model=model)\n",
    "        posterior_transform = self._deprecate_acqf_objective(\n",
    "            posterior_transform=posterior_transform,\n",
    "            objective=kwargs.get(\"objective\"),\n",
    "        )\n",
    "        if posterior_transform is None:\n",
    "            if model.num_outputs != 1:\n",
    "                raise UnsupportedError(\n",
    "                    \"Must specify a posterior transform when using a \"\n",
    "                    \"multi-output model.\"\n",
    "                )\n",
    "        else:\n",
    "            if not isinstance(posterior_transform, PosteriorTransform):\n",
    "                raise UnsupportedError(\n",
    "                    \"AnalyticAcquisitionFunctions only support PosteriorTransforms.\"\n",
    "                )\n",
    "        self.posterior_transform = posterior_transform\n",
    "\n",
    "    def set_X_pending(self, X_pending: Optional[Tensor] = None) -> None:\n",
    "        raise UnsupportedError(\n",
    "            \"Analytic acquisition functions do not account for X_pending yet.\"\n",
    "        )\n",
    "\n",
    "    def _mean_and_sigma(\n",
    "        self, X: Tensor, compute_sigma: bool = True, min_var: float = 1e-12\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Computes the first and second moments of the model posterior.\n",
    "\n",
    "        Args:\n",
    "            X: `batch_shape x q x d`-dim Tensor of model inputs.\n",
    "            compute_sigma: Boolean indicating whether or not to compute the second\n",
    "                moment (default: True).\n",
    "            min_var: The minimum value the variance is clamped too. Should be positive.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of tensors containing the first and second moments of the model\n",
    "            posterior. Removes the last two dimensions if they have size one. Only\n",
    "            returns a single tensor of means if compute_sigma is True.\n",
    "        \"\"\"\n",
    "        self.to(device=X.device)  # ensures buffers / parameters are on the same device\n",
    "        posterior = self.model.posterior(\n",
    "            X=X, posterior_transform=self.posterior_transform\n",
    "        )\n",
    "        mean = posterior.mean.squeeze(-2).squeeze(-1)  # removing redundant dimensions\n",
    "        if not compute_sigma:\n",
    "            return mean, None\n",
    "        sigma = posterior.variance.clamp_min(min_var).sqrt().view(mean.shape)\n",
    "        return mean, sigma\n",
    "\n",
    "\n",
    "# --------------- Helper functions for analytic acquisition functions. ---------------\n",
    "\n",
    "\n",
    "def _scaled_improvement(\n",
    "    mean: Tensor, sigma: Tensor, best_f: Tensor, maximize: bool\n",
    ") -> Tensor:\n",
    "    \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "    u = (mean - best_f) / sigma\n",
    "    return u if maximize else -u\n",
    "\n",
    "\n",
    "def _gamma(\n",
    "    mean: Tensor, sigma: Tensor, fstar: Tensor, maximize: bool\n",
    ") -> Tensor:\n",
    "    \"\"\"Returns `u = (mean - best_f) / sigma`, -u if maximize == True.\"\"\"\n",
    "    u = (fstar - mean) / sigma\n",
    "    return u if maximize else -u\n",
    "\n",
    "\n",
    "def _ei_helper(u: Tensor) -> Tensor:\n",
    "    \"\"\"Computes phi(u) + u * Phi(u), where phi and Phi are the standard normal\n",
    "    pdf and cdf, respectively. This is used to compute Expected Improvement.\n",
    "    \"\"\"\n",
    "    return phi(u) + u * Phi(u)\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.functional import softplus\n",
    "import math\n",
    "\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "def cauchy(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes a Lorentzian, i.e. an un-normalized Cauchy density function.\"\"\"\n",
    "    return 1 / (1 + x.square())\n",
    "\n",
    "def fatplus(x: Tensor, tau: Union[float, Tensor] = 0.5) -> Tensor:\n",
    "    \"\"\"Computes a fat-tailed approximation to `ReLU(x) = max(x, 0)` by linearly\n",
    "    combining a regular softplus function and the density function of a Cauchy\n",
    "    distribution. The coefficient `alpha` of the Cauchy density is chosen to guarantee\n",
    "    monotonicity and convexity.\n",
    "\n",
    "    Args:\n",
    "        x: A Tensor on whose values to compute the smoothed function.\n",
    "        tau: Temperature parameter controlling the smoothness of the approximation.\n",
    "\n",
    "    Returns:\n",
    "        A Tensor of values of the fat-tailed softplus.\n",
    "    \"\"\"\n",
    "\n",
    "    def _fatplus(x: Tensor) -> Tensor:\n",
    "        alpha = 1e-1  # guarantees monotonicity and convexity (TODO: ref + Lemma 4)\n",
    "        return softplus(x) + alpha * cauchy(x)\n",
    "\n",
    "    return tau * _fatplus(x / tau)\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import numpy as np \n",
    "# Define a function to be executed in parallel\n",
    "def logpdf_calculation(x_temp,model,D,fstar):\n",
    "        \n",
    "            \n",
    "            x_temp = x_temp.reshape(1,1,D)\n",
    "            \n",
    "            model_temp = model.get_fantasy_model(x_temp.reshape(-1,D), torch.tensor([fstar.item()]).reshape(-1,1))\n",
    "            model_temp.N = model.N+1\n",
    "            model_temp.train_targets = model_temp.train_targets.reshape(model_temp.N)\n",
    "            \n",
    "            mean_d, variance_d = model_temp.posterior_derivative(x_temp.reshape(-1,1,D))\n",
    "               \n",
    "            variance_d_new = torch.diagonal(variance_d, dim1=-2, dim2=-1)\n",
    "            sigma_d_new = variance_d_new.sqrt()\n",
    "\n",
    "            u = (torch.tensor(0.)-mean_d)/sigma_d_new\n",
    "            \n",
    "            max_dis = 20.\n",
    "            u = fatplus(u+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "            u = - (fatplus(-u+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "\n",
    "            logpdf_temp = torch.sum(log_phi(u),dim=1).item()\n",
    "            \n",
    "                \n",
    "            return logpdf_temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TruncatedExpectedImprovement_GradientEnhanced_fantasy_parallel(AnalyticAcquisitionFunction):\n",
    "   \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        best_f: Union[float, Tensor],\n",
    "        fstar: Union[float, Tensor],\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        maximize: bool = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super().__init__(model=model, posterior_transform=posterior_transform, **kwargs)\n",
    "        self.register_buffer(\"best_f\", torch.as_tensor(best_f))\n",
    "        self.register_buffer(\"fstar\", torch.as_tensor(fstar))\n",
    "        self.maximize = maximize\n",
    "\n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "       \n",
    "        mean, sigma = self._mean_and_sigma(X)\n",
    "        u1 = _scaled_improvement(mean, sigma, self.best_f, self.maximize)\n",
    "        u2 = _scaled_improvement(mean, sigma, self.fstar, self.maximize)\n",
    "        \n",
    "        max_dis = 20.\n",
    "        u1 = fatplus(u1+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "        u1 = - (fatplus(-u1+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "        \n",
    "        max_dis = 20.\n",
    "        u2 = fatplus(u2+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "        u2 = - (fatplus(-u2+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "        \n",
    "        part1 = ((sigma * _ei_helper(u1)) -(sigma * _ei_helper(u2))).log()\n",
    "        \n",
    "        # part 2 calculation\n",
    "        D = X.shape[-1]\n",
    "        partial_logpdf_calculation = partial(logpdf_calculation, model=self.model,D=D,fstar=self.fstar)\n",
    "         \n",
    "        X_temp_list = [X[i, :, :] for i in range(X.size(0))]\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # Map the new function to the data using multiple processes\n",
    "            results = executor.map(partial_logpdf_calculation, X_temp_list)\n",
    "        \n",
    " \n",
    "        # logpdf_total = torch.stack(list(results))\n",
    "     \n",
    "        # part2 = logpdf_total\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 10\n",
    "N = 1\n",
    "function = Branin(negate=True)\n",
    "fstar = -0.397887 \n",
    "acquisition = 'TruncatedExpectedImprovement_GradientEnhanced_fantasy_parallel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-3.5451956715421797\n"
     ]
    }
   ],
   "source": [
    "bounds=function.bounds.to(device)\n",
    "dim = bounds.shape[1]\n",
    "standard_bounds=torch.tensor([0.,1.]*dim).reshape(-1,2).T.to(device)\n",
    "\n",
    "\n",
    "record = []\n",
    "\n",
    "for exp in range(N):\n",
    "    \n",
    "\n",
    "    print(exp)\n",
    "    torch.manual_seed(exp)\n",
    "\n",
    "    train_x_standard = get_initial_points(dim,4*dim,exp).to(device)\n",
    "    train_x = unnormalize(train_x_standard, bounds).reshape(-1,dim)\n",
    "    train_obj = function(train_x).unsqueeze(-1)\n",
    "\n",
    "    best_value = train_obj.max().item()\n",
    "    best_value_holder = [best_value]\n",
    "    \n",
    "    print(best_value_holder[-1])\n",
    "\n",
    "    for i in range (1):\n",
    "\n",
    "        train_x_standard = normalize(train_x, bounds).to(device)\n",
    "        train_obj_standard = (train_obj - train_obj.mean()) / train_obj.std()\n",
    "        fstar_standard = (fstar - train_obj.mean()) / train_obj.std()\n",
    "        \n",
    "        torch.manual_seed(exp+iter_num)\n",
    "        model = DerivativeExactGPSEModel(dim,train_x_standard, train_obj_standard).to(device)    \n",
    "\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model) .to(device)\n",
    "\n",
    "        torch.manual_seed(exp+iter_num)\n",
    "        \n",
    "        try:\n",
    "            fit_gpytorch_mll(mll)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        torch.manual_seed(exp+iter_num)\n",
    "\n",
    "        if acquisition == 'TruncatedExpectedImprovement_GradientEnhanced_fantasy_parallel':\n",
    "            AF = TruncatedExpectedImprovement_GradientEnhanced_fantasy_parallel(model=model, best_f=train_obj_standard.max().item(),fstar=fstar_standard) .to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    #     try:\n",
    "    #         np.random.seed(exp+i)\n",
    "    #         new_point_analytic = My_acquisition_opt(AF,dim)\n",
    "    #         new_point_analytic = torch.tensor(new_point_analytic).reshape(-1,dim).to(device)\n",
    "    #     except:\n",
    "    #         print('problem occur')\n",
    "    #         if acquisition == 'TruncatedExpectedImprovement_GradientEnhanced_fantasy' or acquisition == 'TruncatedExpectedImprovement_GradientEnhanced_fantasy_2':\n",
    "    #             AF_temp = TruncatedExpectedImprovement(model=model, best_f=train_obj_standard.max().item(),fstar=fstar_standard) .to(device)\n",
    "            \n",
    "    #         np.random.seed(exp+i)\n",
    "    #         new_point_analytic = My_acquisition_opt(AF_temp,dim)\n",
    "    #         new_point_analytic = torch.tensor(new_point_analytic).reshape(-1,dim).to(device)\n",
    "        \n",
    "    #     print(new_point_analytic)\n",
    "\n",
    "    #     next_x = unnormalize(new_point_analytic, bounds).reshape(-1,dim)\n",
    "    #     new_obj = function(next_x).unsqueeze(-1) .to(device)\n",
    "\n",
    "\n",
    "    #     train_x = torch.cat((train_x, next_x))\n",
    "    #     train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    #     best_value = train_obj.max().item()\n",
    "    #     best_value_holder.append(best_value)\n",
    "\n",
    "    #     print(best_value_holder[-1])\n",
    "\n",
    "    # best_value_holder = np.array(best_value_holder)\n",
    "    # record.append(best_value_holder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/utils/transforms.py:156\u001b[0m, in \u001b[0;36m_verify_output_shape\u001b[0;34m(acqf, X, output)\u001b[0m\n\u001b[1;32m    155\u001b[0m X_batch_shape \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 156\u001b[0m \u001b[39mif\u001b[39;00m output\u001b[39m.\u001b[39;49mshape \u001b[39m==\u001b[39m X_batch_shape:\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AF(torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0.5\u001b[39;49m,\u001b[39m0.5\u001b[39;49m])\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,dim))\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/utils/transforms.py:290\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(acqf, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m is_fully_bayesian(acqf\u001b[39m.\u001b[39mmodel):\n\u001b[1;32m    289\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m \u001b[39mif\u001b[39;00m assert_output_shape \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _verify_output_shape(\n\u001b[1;32m    291\u001b[0m     acqf\u001b[39m=\u001b[39;49macqf,\n\u001b[1;32m    292\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    293\u001b[0m     output\u001b[39m=\u001b[39;49moutput,\n\u001b[1;32m    294\u001b[0m ):\n\u001b[1;32m    295\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected the output shape to match either the t-batch shape of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mX, or the `model.batch_shape` in the case of acquisition \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunctions using batch models; but got output with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for X with shape \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/botorch/utils/transforms.py:180\u001b[0m, in \u001b[0;36m_verify_output_shape\u001b[0;34m(acqf, X, output)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    177\u001b[0m     \u001b[39m# acqf does not have model or acqf.model does not define `batch_shape`\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    179\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOutput shape checks failed! Expected output shape to match t-batch shape\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof X, but got output with shape \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m for X with shape\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m. Make sure that this is the intended behavior!\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m         \u001b[39mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "AF(torch.tensor([0.5,0.5]).reshape(1,1,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "aa = torch.tensor([[0.5,0.5],[0.3,0.6]]).reshape(2,1,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5000, 0.5000]]), tensor([[0.3000, 0.6000]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[aa[i, :, :] for i in range(aa.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import numpy as np \n",
    "# Define a function to be executed in parallel\n",
    "def logpdf_calculation(x_temp,model,D,fstar):\n",
    "        \n",
    "            \n",
    "            x_temp = x_temp.reshape(1,1,D)\n",
    "            \n",
    "            model_temp = model.get_fantasy_model(x_temp.reshape(-1,D), torch.tensor([fstar.item()]).reshape(-1,1))\n",
    "            model_temp.N = model.N+1\n",
    "            model_temp.train_targets = model_temp.train_targets.reshape(model_temp.N)\n",
    "            \n",
    "            mean_d, variance_d = model_temp.posterior_derivative(x_temp.reshape(-1,1,D))\n",
    "               \n",
    "            variance_d_new = torch.diagonal(variance_d, dim1=-2, dim2=-1)\n",
    "            sigma_d_new = variance_d_new.sqrt()\n",
    "\n",
    "            u = (torch.tensor(0.)-mean_d)/sigma_d_new\n",
    "            \n",
    "            max_dis = 20.\n",
    "            u = fatplus(u+max_dis,tau=0.2)-torch.as_tensor(max_dis) \n",
    "            u = - (fatplus(-u+max_dis,tau=0.2)-torch.as_tensor(max_dis))\n",
    "\n",
    "            logpdf_temp = torch.sum(log_phi(u),dim=1).item()\n",
    "            \n",
    "                \n",
    "            return logpdf_temp.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(processes\u001b[39m=\u001b[39mmp\u001b[39m.\u001b[39mcpu_count())  \u001b[39m# Use the number of available CPUs\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# # # Map the function to the x_temp values using multiple processes\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mstarmap(logpdf_calculation, [(x, model, D, fstar) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m x_temp_values])\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     put(task)\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[39m=\u001b[39m task[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mcls\u001b[39m, obj, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mcls\u001b[39;49m(buf, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m buf\u001b[39m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/anaconda3/envs/known_boundary/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:155\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    152\u001b[0m storage \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39m_typed_storage()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mrequires_grad \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m tensor\u001b[39m.\u001b[39mis_leaf:\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCowardly refusing to serialize non-leaf tensor which requires_grad, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39msince autograd does not support crossing process boundaries.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mIf you just want to transfer the data, call detach() on the tensor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mbefore serializing (e.g., putting it on the queue).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m check_serializing_named_tensor(tensor)\n\u001b[1;32m    161\u001b[0m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mhooks\u001b[39m.\u001b[39mwarn_if_has_hooks(tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "aa = torch.tensor([[0.5,0.5],[0.3,0.6]]).reshape(2,1,dim)\n",
    "x_temp_values =[aa[i, :, :] for i in range(aa.size(0))] # List of x_temp values\n",
    "model = model  # Your model object\n",
    "D = 2  # D value\n",
    "fstar = fstar # fstar value\n",
    "\n",
    "pool = mp.Pool(processes=mp.cpu_count())  # Use the number of available CPUs\n",
    "\n",
    "# # # Map the function to the x_temp values using multiple processes\n",
    "results = pool.starmap(logpdf_calculation, [(x, model, D, fstar) for x in x_temp_values])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "known_boundary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
